{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-Levenshtein"
      ],
      "metadata": {
        "id": "q_xqyQK4UoLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer pandas"
      ],
      "metadata": {
        "id": "Uo2_F90-UnpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import jiwer\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"/content/750_test_2025-06-17_15-51-29 - Sheet1.csv\")\n",
        "# Display the first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "sStWR9J7UlfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAfG1ad9UfdC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from jiwer import wer, cer\n",
        "\n",
        "# Load your CSV (assumes df is already defined; update if needed)\n",
        "# df = pd.read_csv(\"your_file.csv\")  # Uncomment and change if needed\n",
        "\n",
        "# Extract ground truth texts\n",
        "ground_truths = df[\"Ground Truth\"].astype(str).tolist()\n",
        "\n",
        "# List of model columns\n",
        "models = [\"Predictions\"]  # Update this if using multiple models\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Define custom metric function\n",
        "def compute_word_metrics(ground_truths, predictions):\n",
        "    tp, fp, fn = 0, 0, 0\n",
        "\n",
        "    for gt, pred in zip(ground_truths, predictions):\n",
        "        gt_words = set(gt.split())  # Tokenize ground truth\n",
        "        pred_words = set(pred.split())  # Tokenize prediction\n",
        "\n",
        "        tp += len(gt_words & pred_words)              # True Positives\n",
        "        fp += len(pred_words - gt_words)              # False Positives\n",
        "        fn += len(gt_words - pred_words)              # False Negatives\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return precision * 100, recall * 100, f1_score * 100\n",
        "\n",
        "# Loop through models\n",
        "for model in models:\n",
        "    predictions = df[model].astype(str).tolist()\n",
        "\n",
        "    # Filter out empty or whitespace-only entries\n",
        "    filtered_pairs = [(gt, pred) for gt, pred in zip(ground_truths, predictions) if gt.strip() and pred.strip()]\n",
        "\n",
        "    if not filtered_pairs:\n",
        "        print(f\"Skipping model '{model}' due to empty or invalid data.\")\n",
        "        continue\n",
        "\n",
        "    ground_truths_cleaned, predictions_cleaned = zip(*filtered_pairs)\n",
        "\n",
        "    # Compute WER and CER\n",
        "    wer_value = wer(list(ground_truths_cleaned), list(predictions_cleaned))\n",
        "    cer_value = cer(list(ground_truths_cleaned), list(predictions_cleaned))\n",
        "\n",
        "    # Compute Accuracy\n",
        "    exact_matches = sum(gt == pred for gt, pred in zip(ground_truths_cleaned, predictions_cleaned))\n",
        "    accuracy = (exact_matches / len(ground_truths_cleaned)) * 100\n",
        "\n",
        "    # Compute Precision, Recall, and F1-score\n",
        "    precision, recall, f1_score = compute_word_metrics(ground_truths_cleaned, predictions_cleaned)\n",
        "\n",
        "    # Append results\n",
        "    results.append([model, wer_value, cer_value, accuracy, precision, recall, f1_score])\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"WER\", \"CER\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\n",
        "print(results_df)\n"
      ]
    }
  ]
}